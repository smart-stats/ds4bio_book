{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb0bf21",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/smart-stats/ds4bio_book/blob/main/book/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396c8b3",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "Pytorch is a widely used library for fitting artificial intelligence models. At its core, pytorch is a **tensor** library. Tensors are simply multidimensional arrays. Also, since `numpy` is another core library for manipulating tensors, `pytorch` has several methods dedicated for conversion.\n",
    "\n",
    "First let's create a basic tensor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afdde61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Creating Tensors from data\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653a2da",
   "metadata": {},
   "source": [
    "Not unlike `numpy`, `pytorch` has utilities for filling empty arrays with constant or random values. Here `rand` refers to random uniforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5033bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.7178, 0.4327, 0.2454],\n",
      "        [0.3034, 0.6983, 0.2065]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "rand_tensor = torch.rand(shape)  # Random numbers from uniform distribution [0, 1)\n",
    "ones_tensor = torch.ones(shape)  # All ones\n",
    "\n",
    "print(ones_tensor)\n",
    "print(rand_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7fb2f6",
   "metadata": {},
   "source": [
    "As mentioned, it is easy to switch between `numpy` and `pytorch`. Notice that the `torch` tensor is not a copy of the `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68a6d45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30]\n",
      "tensor([10, 20, 30])\n",
      "[100  20  30]\n",
      "tensor([100,  20,  30])\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array([10, 20, 30]) ## Create a numpy array\n",
    "t_from_np = torch.from_numpy(np_array) ## Create a tensor from the numpy array\n",
    "print(np_array)\n",
    "print(t_from_np)\n",
    "np_array[0] = 100 ## Modify the numpy array\n",
    "print(np_array)\n",
    "print(t_from_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf937cbd",
   "metadata": {},
   "source": [
    "`pytorch` has array arithmetic operations, such as matrix operations. Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52bcae01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions [torch.Size([2, 2]), torch.Size([2, 2])]\n",
      "Element-wise multiplication:\n",
      " tensor([[ 5, 12],\n",
      "        [21, 32]])\n",
      "Matrix Multiplication:\n",
      " tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "Tensor A after in-place addition:\n",
      " tensor([[6, 7],\n",
      "        [8, 9]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor_b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "print(\"Dimensions\", [tensor_a.shape, tensor_b.shape])\n",
    "\n",
    "# Element-wise multiplication\n",
    "print(\"Element-wise multiplication:\\n\", tensor_a * tensor_b)\n",
    "\n",
    "# Matrix Multiplication, similar to numpy \n",
    "# Can use .matmul() or the @ operator\n",
    "print(\"Matrix Multiplication:\\n\", tensor_a @ tensor_b)\n",
    "\n",
    "# In-place operations (denoted by an underscore)\n",
    "# This modifies tensor_a directly rather than creating a new copy\n",
    "tensor_a.add_(5)\n",
    "print(\"Tensor A after in-place addition:\\n\", tensor_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c165f",
   "metadata": {},
   "source": [
    "\"Broadcasting\" allows PyTorch to perform operations on tensors of different shapes. It automatically \"stretches\" the smaller tensor to match the larger one without copying data. This is similar to R, which automatically does this in its base matrix algebra libraries. It's easy to get mixed up with these sorts of calculations, and to mess up transposes. So test your code thoroughly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc025ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions [torch.Size([2, 3]), torch.Size([3])]\n",
      "Broadcasting result:\n",
      " tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]]) \n",
    "vector = torch.tensor([10, 20, 30])\n",
    "print(\"Dimensions\", [matrix.shape, vector.shape])\n",
    "\n",
    "## Notice how the vector is duplicated \n",
    "result = matrix + vector\n",
    "\n",
    "print(\"Broadcasting result:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30b114",
   "metadata": {},
   "source": [
    "Perhaps `pytorch`'s best attribute is its ability to calculate derivatives. Let's go through a simple example where we can calculate the derivative ourselves. Here `a` and `b` are the variables of integration with the specific values given (2 and 6). The `require's_grad=True` option is telling `pytorch` to keep track of the gradient. The `backward` method is calculating the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d90d6d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q value: -12.0\n",
      "Computed dQ/da: 36.0\n",
      "Computed dQ/db: -12.0\n"
     ]
    }
   ],
   "source": [
    "# 'requires_grad=True' tells PyTorch to track every operation on these tensors\n",
    "a = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([6.0], requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "print(f\"Q value: {Q.item()}\") # (3 * 8) - 36 = 24 - 36 = -12\n",
    "\n",
    "# Compute gradients (Backpropagation)\n",
    "# This traverses the graph backwards to calculate derivatives\n",
    "Q.backward()\n",
    "\n",
    "# Check the results against analytic calculus\n",
    "# Expected dQ/da = 9 * (2^2) = 36\n",
    "print(f\"Computed dQ/da: {a.grad.item()}\")\n",
    "\n",
    "# Expected dQ/db = -2 * 6 = -12\n",
    "print(f\"Computed dQ/db: {b.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fedc53a",
   "metadata": {},
   "source": [
    "The matrices can be far more complex. That's where we'll use them when building our AI models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
