{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unsupervised learning\n",
        "## PCA\n",
        "\n",
        "Let $\\{X_i\\}$ for $i=1,\\ldots,n$ be $p$ random vectors with means\n",
        "$(0,\\ldots,0)^t$ and variance matrix $\\Sigma$. Consider finding $v_1$,\n",
        "a $p$ dimensional vector with $||v_1|| = 1$ so that $v_1^t \\Sigma v_1$\n",
        "is maximized. Notice this is equivalent to saying we want to maximize\n",
        "$\\mathrm{Var}( X_i^t V_1)$. The well known solution to this equation\n",
        "is that $v_1$ is the first eigenvector of $\\Sigma$ and $\\lambda_1 =\n",
        "\\mathrm{Var}( X_i^t V_1)$ is the associated eigenvalue. If $\\Sigma =\n",
        "V^t \\Lambda V$ is the eigenvalue decomposition of where $V$ are the\n",
        "eigenvectors and $\\Lambda$ is a diagonal matrix of the eigenvalues\n",
        "ordered from greatest to least, then $v_1$ corresponds to the first\n",
        "column of $V$ and $\\lambda_1$ corresponds to the first element of\n",
        "$\\Lambda$. If one then finds $v_k$ as the vector maximizing $v_k^t\n",
        "\\Sigma v_k$ so that $v_k^t v_{k'} = I(k=k')$, then the $v_k$ are the\n",
        "columns of $V$ and $v_k^t \\Sigma v_k = \\lambda_k$ are the eigenvalues.\n",
        "\n",
        "Notice:\n",
        "\n",
        "1. $V \\Sigma V^t = \\Lambda$ (i.e. $V$ diagonalizess $\\Sigma$)\n",
        "2. $\\mbox{Trace}(\\Sigma) = \\mbox{Trace}(\\Sigma V^t V) = \\mbox{Trace}(V \\Sigma V^t) = \\sum \\lambda_k$ (i.e. the total variability is the sum of the eigenvalues)\n",
        "3. Since $V^t V = I$, $V$ is a rotation matrix. Thus, $V$ rotates $X_i$ in such a way that to maximize variability in the first dimension, then the second dimensions ...\n",
        "4. $\\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} )= \\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} )\n",
        "v_k^t \\mbox{Cov}(x_i, x_i^t) v_{k'} =  v_k^t V v_{k'} = 0$ if $k\\neq k'$\n",
        "5. Another representation of $\\Sigma$ is $\\sum_{k=1}^p \\lambda_i v_k v_k^t$ by simply rewriting the matrix algebra of $V \\Lambda V^t$.\n",
        "6. The variables $U_i = V X_i$ then: have uncorrelated elements ($\\mbox{Cov}(U_{ik}, U_{ik'}) = 0$ for $k\\neq k'$ by property 5), have the same total variability as the elements of $X_i$ ($\\sum_k \\mbox{Var}(U_{ik}) = \\sum_k \\lambda_k = \\sum_k \\mbox{Var}(X_{ik})$ by property 2), are a rotation of the $X_i$, are ordered so that $U_{i1}$ has the greatest amount of variability and so on.  \n",
        "\n",
        "Notation:\n",
        "\n",
        "1. The $\\lambda_k$ are simply called the eigenvalues or principal components variation.\n",
        "2. $U_{ik} = X_i^t v_k$ is called the **principal component scores**.\n",
        "3. The $v_k$ are called the **principal component loadings** or **weights**, with $v_1$ being called the first principal component and so on.\n",
        "\n",
        "Statistical properties under the assumption that the $x_i$ are iid with mean 0 and variance $\\Sigma$ \n",
        "\n",
        "1. $E[U_{ik}]=0$\n",
        "2. $\\mbox{Var}(U_{ik}) = \\lambda_k$\n",
        "3. $\\mbox{Cov}(U_{ik}, U_{ik'}) = 0$ if $k\\neq k'$\n",
        "4. $\\sum_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Trace}(\\Sigma)$.\n",
        "5. $\\prod_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Det}(\\Sigma)$\n",
        "\n",
        "### Sample PCA\n",
        "\n",
        "Of course, we're describing PCA as a conceptual process. We realize\n",
        "$n$ $p$ dimensional vectors $x_1$ to $x_n$, typically organized in $X$\n",
        "a $n\\times p$ matrix. If $X$ is not mean 0, we typically demean it by\n",
        "calculating $(I- J(J^t J)^{-1} J') X$ where $J$ is a vector of\n",
        "ones. Assume this is done. Then $\\frac{1}{n-1} X^t X = \\hat\n",
        "\\Sigma$. Thus, our sample PCA is obtained via the eigenvalue\n",
        "decomposition $\\hat \\Sigma = \\hat V \\hat \\Lambda \\hat V^t$ and our\n",
        "principal components obtained as $ X V$.\n",
        "\n",
        "We can relate PCA to the SVD as follows. Let $\\frac{1}{\\sqrt{n-1}} X =\n",
        "\\hat U \\hat \\Lambda^{1/2} \\hat V^t$ be the SVD of the scaled version\n",
        "of $X$. Then note that $$ \\hat \\Sigma = \\frac{1}{n-1} X^t X = \\hat V\n",
        "\\hat \\Lambda \\hat V^t $$ yields the sample covariance matrix\n",
        "eigenvalue decomposition.\n",
        "\n",
        "\n",
        "### PCA with a large dimension\n",
        "\n",
        "Consider the case where one of $n$ or $p$ is large. Let's assume $n$ is large. Then\n",
        "$$\n",
        "\\frac{1}{n-1} X^t X = \\frac{1}{n-1} \\sum_i x_i x_i^t\n",
        "$$\n",
        "As we learned in the chapter on HDF5, we can do sums like these without loading the entirety of $X$ into memory. Thus, in this case, we can calculate the eigenvectors using only the small dimension. If, on the other hand, $p$ is large and $n$ is smaller, then we can calculate the\n",
        "eigenvalue decomposition of\n",
        "$$\n",
        "\\frac{1}{n-1} X X^t = \\hat U \\hat \\Lambda \\hat U^t.\n",
        "$$\n",
        "In either case, whether $U$ or $V$ is easier to get, we can then obtain the other via vectorized multiplication.\n",
        "\n",
        "\n",
        "### Simple example\n"
      ],
      "id": "d9f09b0b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as la\n",
        "from sklearn.decomposition import PCA\n",
        "import urllib.request\n",
        "import PIL\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.decomposition import FastICA\n",
        "from tqdm import tqdm\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "import scipy\n",
        "import IPython"
      ],
      "id": "71cb7a50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = 1000\n",
        "mu = (0, 0)\n",
        "Sigma = np.matrix([[1, .5], [.5, 1]])\n",
        "X = np.random.multivariate_normal( mean = mu, cov = Sigma, size = n)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1])"
      ],
      "id": "a3788536",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = X - X.mean(0)\n",
        "print(X.mean(0))\n",
        "Sigma_hat = np.matmul(np.transpose(X), X) / (n-1) \n",
        "Sigma_hat"
      ],
      "id": "8e9cade4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "evd = la.eig(Sigma_hat)\n",
        "lambda_ = evd[0]\n",
        "v_hat = evd[1]\n",
        "u_hat = np.matmul(X, np.transpose(v_hat))\n",
        "plt.scatter(u_hat[:,0], u_hat[:,1])"
      ],
      "id": "aaf8f6f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit using scikitlearn's function\n"
      ],
      "id": "552b0612"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pca = PCA(n_components = 2).fit(X)\n",
        "print(pca.explained_variance_)\n",
        "print(lambda_ )"
      ],
      "id": "c235c563",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example\n",
        "\n",
        "Let's consider the melanoma dataset that we looked at before. First we read in the data as we have done before so we don't show that code.\n"
      ],
      "id": "e91b9209"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "data_flag = 'dermamnist'\n",
        "\n",
        "## This defines our NN parameters\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "lr = 0.001\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "##https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb\n",
        "data_flag = 'dermamnist'\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "  \n",
        "  transforms.ToTensor()\n",
        "\n",
        "])\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split = 'train', transform = data_transform, download = True)\n",
        "test_dataset  = DataClass(split = 'test' , transform = data_transform, download = True)\n",
        "pil_dataset   = DataClass(split = 'train',                             download = True)\n",
        "\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
      ],
      "id": "b021113e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's get the data from the torch dataloader format back into an image array and a matrix with the image part (28, 28, 3) vectorized.\n"
      ],
      "id": "e92785d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def loader_to_array(dataloader):\n",
        "  ## Read one iteration to get data\n",
        "  test_input, test_target = iter(dataloader).next()\n",
        "  ## Total number of training images\n",
        "  n = np.sum([inputs.shape[0] for inputs, targets in dataloader])\n",
        "  ## The dimensions of the images\n",
        "  imgdim = (test_input.shape[2], test_input.shape[3])\n",
        "  images = np.empty( (n, imgdim[0], imgdim[1], 3))\n",
        "\n",
        "  ## Read the data from the data loader into our numpy array\n",
        "  idx = 0\n",
        "  for inputs, targets in dataloader:\n",
        "    inputs = inputs.detach().numpy()\n",
        "    for j in range(inputs.shape[0]):\n",
        "      img = inputs[j,:,:,:]\n",
        "      ## get it out of pytorch format\n",
        "      img = np.transpose(img, (1, 2, 0))\n",
        "      images[idx,:,:,:] = img\n",
        "      idx += 1\n",
        "  matrix = images.reshape(n, 3 * np.prod(imgdim))\n",
        "  return images, matrix\n",
        "\n",
        "train_images, train_matrix = loader_to_array(train_loader)\n",
        "test_images, test_matrix = loader_to_array(test_loader)\n",
        "\n",
        "## Demean the matrices\n",
        "train_mean = train_matrix.mean(0)\n",
        "train_matrix = train_matrix - train_mean\n",
        "test_mean = test_matrix.mean(0)\n",
        "test_matrix = test_matrix - test_mean"
      ],
      "id": "fcba477d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's actually perform PCA using scikitlearn. We'll plot the eigenvalues divided by their sums, $\\lambda_k / \\sum_{k'} \\lambda_{k'}$. This is called a scree plot.\n"
      ],
      "id": "c6fd3d8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.decomposition import PCA\n",
        "n_comp = 10\n",
        "pca = PCA(n_components = n_comp).fit(train_matrix)\n",
        "plt.plot(pca.explained_variance_ratio_)"
      ],
      "id": "278c7308",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Often this is done by plotting the cummulative sum so that you can visualize how much variance is explained by including the top $k$ components. Here I fit 10 components and they explain 85% of the variation.\n"
      ],
      "id": "fe1bcb0d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))"
      ],
      "id": "2ef93a2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the weights from the eigenvectors, $V$, are images. We can plot these as images.\n"
      ],
      "id": "893adf37"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eigen_moles = pca.components_"
      ],
      "id": "af6f3fbb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in range(10): \n",
        "  plt.subplot(2, 5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  img = eigen_moles[i,:].reshape(28, 28, 3)\n",
        "  img = (img - img.min())\n",
        "  img = img / img.max()\n",
        "  img = img * 255 \n",
        "  img = img.astype(np.uint8)\n",
        "  plt.imshow(img)"
      ],
      "id": "1aee7ead",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's project our testing data onto the principal component basis\n",
        "created by our training data and see how it does. Let $X_{training} =\n",
        "U \\Lambda^{1/2} V^t$ is the SVD of our training data.  Then, we can\n",
        "convert ths scores, $U$ back to $X_{training}$ with the map $U\n",
        "\\rightarrow U \\lambda^{1/2} V$. Or, if our scores are normalized, $U\n",
        "\\Lambda^{1/2}$ then we simply multiply by $V^t$. If we want to\n",
        "represent $X_{training}$ by a lower dimensional summary, we just keep\n",
        "fewer columns of scores, then multiply by the same columns of $V$. We\n",
        "could write this as $U_s = X_{training} V_S \\lambda^{-1/2}_S$, where\n",
        "$S$ refers to a subset of values of $k$.\n",
        "\n",
        "Notice that \n",
        "$\\hat X_{training} = U_{S} V^t_S \\Lambda^{-1/2}_S = X_{training} V_S V_S^t$ , $\\Lambda$ and $V$. Consider then an approximation to $X_{test}$ as\n",
        "$\\hat X_{test} = X_{test} V_s V_S^t$. Written otherwise\n",
        "$$\n",
        "\\hat X_{i,test} = \\sum_{k \\in S} <x_{i,test}, v_k> v_k\n",
        "$$\n",
        "which is the projection of subject $i$'s features into the linear space spanned by the basis defined by the principal component loadings. \n",
        "\n",
        "Let's try this on our mole data.\n"
      ],
      "id": "34cb846b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test_matrix_fit = pca.inverse_transform(pca.transform(test_matrix))\n",
        "np.mean(np.abs( test_matrix - test_matrix_fit))"
      ],
      "id": "3876f2f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "test_matrix_fit_remeaned = test_matrix_fit + test_mean\n",
        "test_matrix_remeaned = test_matrix + test_mean\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in range(5): \n",
        "  plt.subplot(2, 5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  img = test_matrix_remeaned[i,:].reshape(28, 28, 3)\n",
        "  img = (img - img.min())\n",
        "  img = img / img.max()\n",
        "  img = img * 255 \n",
        "  img = img.astype(np.uint8)\n",
        "  plt.imshow(img)\n",
        "\n",
        "  plt.subplot(2, 5,i+6)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  img = test_matrix_fit_remeaned[i,:].reshape(28, 28, 3)\n",
        "  img = (img - img.min())\n",
        "  img = img / img.max()\n",
        "  img = img * 255 \n",
        "  img = img.astype(np.uint8)\n",
        "  plt.imshow(img)"
      ],
      "id": "28cb17f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ICA\n",
        "\n",
        "ICA, independent components analysis, tries to find linear transformations of the data that are statistically independent. Usually, independence is an assumption in ICA, not actually embedded in the loss function. \n",
        "\n",
        "Let $S_t$ be an $\\mathbb{R}^p$ collection of $p$ source signals. Assume that the underlying signals are independent, so that $S_t \\sim F = F_1 \\times F_2 \\times \\ldots \\time f_p$. Assume that the observed data is $X_t = M S_t$ and $X_t \\sim G$. It is typically\n",
        "assumed that $M$ is invertible so that $S_t = M^{-1} X_t$ and $M$ and $M^{-1}$ are called the mixing and unmixing matrices respectively. Note that, since we observe $X_t$ over many repititions of $t$, we can get an estimate of $G$. Typically, it is also assumed that the $X_t$ are iid over $t$.\n",
        "\n",
        "One way to characterize the estimation problem is to parameterize $F_1$, $F_2$ and $F_3$ and use maximum likelihood, or equivalent [citations]. Another is to minimize some distance between $G$ and $F_1$, $F_2$ and $F_3$. Yet another is to actually maximize independence between the components of $S_t$ using some estimate of independence [cite Matteson]. \n",
        "\n",
        "The most popular approaches try to find $M^{-1}$ by maximizing non-Gaussianity. The logic goes that 1) interesting features tend to be non-Gaussian and 2) an appeal to the CLT over signals suggest that the mixed signals should be more Gaussian by being linear combinations of independent things. The latter claim is heuristic relative to the formal CLT. However, maximizing non-Gaussian components tends to work well in practice, thus validating the motivation empirically.\n",
        "\n",
        "One form of ICA maximizes the kurtosis. If $Y$ is a random variable, then \n",
        "$E[Y^4] - 3 E[Y^2]$ is the kurtosis. One could then find $M^{-1}$ that maximizes the empirical kurtosis of the unmixed signals.\n",
        "Another variation of non-Gaussianity maximizes neg-entropy. The neg-entropy of a density $h$ is given by\n",
        "$$\n",
        "- \\int h(y) \\log(h(y)) dy = - E_h[\\log h(Y)]\n",
        "$$\n",
        "A well known theorem states that the Gaussian distribution has the largest entropy of all distributions with the same variance. \n",
        "Therefore, to maximize non-Gaussianity, we can minmize entropy, or equivalently maximize neg-entropy. We could subtract the\n",
        "entropy of the Gaussian distribution to consider this a cross entropy problem, but that only adds a constant to the loss function.\n",
        "The maximization of neg-entropy can be done many ways. We need the following. For a given $M^{-1}$, estimate $G$ from the collection\n",
        "$M^{-1} X_t$, then calculate the neg-entropy of $f_j$. Use that to then take an opimization step of $M$ is the right direction. \n",
        "Some versions of estimation use a polynomial expansion of the $f_j$, which then typically only requires higher order moments, like kurtosis.\n",
        "Fast ICA is a particular implmementation of maximizing neg-entropy.\n",
        "\n",
        "Statistical versions of ICA don't require $M$ to be invertible. Moreover, error terms can be added in which case you can see the connection between ICA and factor analytic models. However, factor analysis  models tend to assume Gaussianity. \n",
        "\n",
        "### Example\n",
        "Consider an example that PCA would have somewhat of a hard time with. In this case, our data is from a mixture of normals with half from a normal with a strong positive correlation and half with a strong negative correlation. Because the angle between the two is not 90 degrees PCA has no chance. No rotation of the axes satisfies the obvious structure in this data.\n"
      ],
      "id": "e63f4804"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = 1000\n",
        "\n",
        "Sigma = np.matrix([[4, 1.8], [1.8, 1]])\n",
        "a = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\n",
        "Sigma = np.matrix([[4, -1.8], [-1.8, 1]])\n",
        "b = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\n",
        "x = np.append( a, b, axis = 0)\n",
        "plt.scatter(x[:,0], x[:,1])\n",
        "plt.xlim([-6, 6])\n",
        "plt.ylim([-6, 6])"
      ],
      "id": "329577e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try fast ICA. Notice it comes much closer to discovering the structure we'd like to discover than PCA could. It pulls appart the two components to a fair degree. Also note, there's a random starting point of ICA, so that I get fairly different fits over re-runs of the algorithm. I had to lower the tolerance to get a good fit.\n",
        "\n",
        "Indpendent components are order invariant and sign invariant.\n"
      ],
      "id": "75cebc10"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transformer = FastICA(tol = 1e-7)\n",
        "icafit = transformer.fit(x)\n",
        "s = icafit.transform(x)\n",
        "plt.scatter(s[:,0], s[:,1])\n",
        "plt.xlim( [s.min(), s.max()])\n",
        "plt.ylim( [s.min(), s.max()])"
      ],
      "id": "fdb6179d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cocktail party example\n",
        "\n",
        "The classic ICA problem is the so called cocktail party problem. In this, you have $p$ sources and $p$ microphones. The microphones each pick up a mixture of signals from the different sources. The goal is to unmix the sources into the components. Independence makes sense in the cocktail party example, since logically conversations would have some independence. \n"
      ],
      "id": "709d11b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import audio2numpy as a2n\n",
        "s1, i1 = a2n.audio_from_file(\"mp3s/4.mp3\")\n",
        "s2, i2 = a2n.audio_from_file(\"mp3s/2.mp3\")\n",
        "s3, i3 = a2n.audio_from_file(\"mp3s/6.mp3\")\n",
        "\n",
        "## Get everything to be the same shape and sum the two audio channels\n",
        "n = np.min((s1.shape[0], s2.shape[0], s3.shape[0]))\n",
        "s1 = s1[0:n,:].mean(axis = 1)\n",
        "s2 = s2[0:n,:].mean(axis = 1)\n",
        "s3 = s3[0:n,:].mean(axis = 1)\n",
        "\n",
        "s = np.matrix([s1, s2, s3])"
      ],
      "id": "2d218760",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "IPython.display.Audio(s1, rate = i1)"
      ],
      "id": "4b17b6c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "IPython.display.Audio(s2, rate = i1)"
      ],
      "id": "2208b17e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "IPython.display.Audio(s3, rate = i1)"
      ],
      "id": "853f1821",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mix the signals.\n"
      ],
      "id": "8e742382"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "w = np.matrix( [ [.7, .2, .1], [.1, .7, .2], [.2, .1, .7] ])\n",
        "x = np.transpose(np.matmul(w, s))"
      ],
      "id": "687a14ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's an example mixed signal\n"
      ],
      "id": "700b4d48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "IPython.display.Audio(data = x[:,1].reshape(n), rate = i1)"
      ],
      "id": "51508878",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now try to unmix using fastICA\n"
      ],
      "id": "e6311e1b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transformer = FastICA(whiten=True, tol = 1e-7)\n",
        "icafit = transformer.fit(x)"
      ],
      "id": "a86d696e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "icafit.mixing_"
      ],
      "id": "8066fa00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unmixing matrix\n"
      ],
      "id": "fdfb4dff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "icafit.components_"
      ],
      "id": "d4e2e2c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a scatterplot matrix where the real component is on the rows and the estimated component is on the columns.\n"
      ],
      "id": "cbec3621"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hat_s = np.transpose(icafit.transform(x))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "    plt.subplot(3, 3, (3 * i + j) + 1)\n",
        "    plt.scatter(hat_s[i,:].squeeze(), np.asarray(s)[j,:])"
      ],
      "id": "572dbd9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now play the estimated sources and see how they turned out.\n"
      ],
      "id": "6f2db408"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.io.wavfile import write\n",
        "i = 0\n",
        "data = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\n",
        "IPython.display.Audio(data = data, rate = i1)"
      ],
      "id": "c2ddfe95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "i = 1\n",
        "data = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\n",
        "IPython.display.Audio(data = data, rate = i1)"
      ],
      "id": "b0f21cec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "i = 2\n",
        "data = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\n",
        "IPython.display.Audio(data = data, rate = i1)"
      ],
      "id": "bfbf93c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imaging example using ICA\n",
        "\n",
        "Let's see what we get for the images. Logically, one would consider\n",
        "voxels as mixed sources and images as the iid replications. But, then\n",
        "the sources would not be images. Let's try the other dimension and see\n",
        "what we get where subject images are mixtures of source images. This\n",
        "is analogous to finding a soure basis of subject images.\n",
        "\n",
        "This is often done in ICA where people transpose matrices to investigate different problems.\n"
      ],
      "id": "c0b56758"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transformer = FastICA(n_components=10, random_state=0,whiten='unit-variance', tol = 1e-7)\n",
        "icafit = transformer.fit_transform(np.transpose(train_matrix))\n",
        "icafit.shape"
      ],
      "id": "5e2d5407",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "train_matrix_remeaned = train_matrix + train_mean\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in range(10): \n",
        "  plt.subplot(2, 5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  img = icafit[:,i].reshape(28, 28, 3)\n",
        "  img = (img - img.min())\n",
        "  img = img / img.max()\n",
        "  img = img * 255 \n",
        "  img = img.astype(np.uint8)\n",
        "  plt.imshow(img)  "
      ],
      "id": "8afdc9dc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}