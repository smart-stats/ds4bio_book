{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM API calls\n",
        "\n",
        "Companies have created APIs to access their large language models. Here we illustrate how to use the Gemini API from Google and GPT from OpenAI. Here we'll show you how to do this on colab where we use its facility for storing secrets. If you're using you need to find an appropriate way to store your API keys securely. My recommendation is to use environment variables. *Never hardcode your API keys in your code*, share them publicly, or commit them to version control. People create bots that scan public repos for API keys to exploit them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te8COTW5VZTQ"
      },
      "source": [
        "# Using Gemini API calls\n",
        "\n",
        "+ First go to [aistudio.google.com](https://aistudio.google.com)\n",
        "+ Create an API key, save this key as a secret in Colab\n",
        "+ The following may not be necessary for free tier\n",
        "    + You have to sign up for a google cloud account\n",
        "    + Create a project\n",
        "    + Enable billing! (Note this cost money so set up an account budget and alerts)\n",
        "+ Pricing can be found [here](https://ai.google.dev/gemini-api/docs/pricing)\n",
        "\n",
        "*Again, make sure to store your API key securely!* and make sure that you aren't creating a large bill for yourself!\n",
        "(Google has some free tier usage but be careful.)\n",
        "\n",
        "First, let's install the Gemini client library. Run this cell in your notebook (only needs to be run once):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpAkAPd71Xvc"
      },
      "outputs": [],
      "source": [
        "# @title Install the generative ai interface\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following fetches the API key and then enters it into the gemini client configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyUiRiB7WOMk"
      },
      "outputs": [],
      "source": [
        "# @title Set up your key with the colab notebook session\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve your API key from Colab's Secrets\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following lists the available models. Refer back to the model list from Gemini to see which models are available to you and their pricing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "L7-P5K14Wgaa",
        "outputId": "19c0c3ac-c3ee-4b53-c14e-c932714853be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n"
          ]
        }
      ],
      "source": [
        "# @title list available commands\n",
        "for m in genai.list_models():\n",
        "  if \"generateContent\" in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's try generating some response text from a prompt. Notice we give a specific model from our model list and then generate the response witih `model.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "mpg158AvWr7t",
        "outputId": "16075b7c-dd5f-48a9-be03-c692eb489c9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Woodchucks, while undeniably cute, are not ideal pets for most people.  Here are some significant downsides:\n",
            "\n",
            "* **Wild Animal Behavior:** Woodchucks are wild animals.  They retain strong instincts to burrow, forage, and defend their territory.  This means they're unlikely to be cuddly or easily trained like a domesticated animal. Expect digging, chewing, and potentially aggressive behavior if they feel threatened.\n",
            "\n",
            "* **Extensive Housing Requirements:**  A suitable enclosure for a woodchuck is far larger than a typical pet cage. You'd need a sizable, secure outdoor enclosure with plenty of space for digging, climbing, and hiding.  This requires significant space, time, and money to build and maintain.  Escape-proofing is paramount.\n",
            "\n",
            "* **Specialized Diet:**  Feeding a woodchuck correctly requires research and commitment. Their diet consists of grasses, plants, fruits, and vegetables â€“ not standard pet store food. Providing a balanced diet and ensuring access to fresh water is crucial.\n",
            "\n",
            "* **Veterinary Care:** Finding a veterinarian experienced with woodchucks can be challenging.  Treatment costs can be high, and illness in a wild animal can be difficult to diagnose and treat.\n",
            "\n",
            "* **Legal Restrictions:**  Keeping wild animals as pets is often restricted by local laws and regulations.  You may need permits or licenses, and it's important to comply with all applicable regulations.\n",
            "\n",
            "* **Disease Risk:**  Woodchucks can carry diseases, such as Lyme disease, that can be transmitted to humans.  Proper hygiene and sanitation are essential, but risk remains.\n",
            "\n",
            "* **Smell:**  Woodchucks, especially males, can have a strong musky odor.\n",
            "\n",
            "* **Destructive Behavior:**  Their natural instinct to dig and chew can cause significant damage to your property if not contained properly.\n",
            "\n",
            "* **Limited Interaction:**  Unlike dogs or cats, you'll likely have limited opportunities for genuine interaction and bonding with a woodchuck.\n",
            "\n",
            "* **Ethical Considerations:**  Some people argue against keeping wild animals as pets, believing it's unnatural and potentially harmful to their well-being.  Removing them from their natural habitat can negatively impact their overall health and longevity.\n",
            "\n",
            "\n",
            "In short, while the novelty might be appealing, the responsibilities and challenges of keeping a woodchuck as a pet far outweigh the potential rewards for most people.  It's generally recommended to appreciate these animals in their natural habitat rather than attempting to domesticate them.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title try running a command\n",
        "model = genai.GenerativeModel('gemini-1.5-flash') # Or gemini-1.5-pro, etc.\n",
        "response = model.generate_content(\"List out the downsides of having a woodchuck as a pet\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's see how to generat embeddings using Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_Uja4iIXppo"
      },
      "outputs": [],
      "source": [
        "# Choose an embedding model\n",
        "embedding_model = 'text-embedding-004' # A good general-purpose embedding model\n",
        "\n",
        "# Text to embed\n",
        "text_to_embed = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Get the embedding\n",
        "response = genai.embed_content(\n",
        "    model=embedding_model,\n",
        "    content=text_to_embed,\n",
        "    #task_type=types.TaskType.SEMANTIC_SIMILARITY # Example task type\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here's the output. Notice it's 768 dimensional. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib7zfVkSYfYj",
        "outputId": "6815a2d0-7957-427c-cd24-6a36c7fd5e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "768\n",
            "[-0.06261901, 0.008358474, 0.020931892, 0.023453966, -0.03660129, 0.033054803, 0.016852979, 0.036087364, 0.047807004]\n"
          ]
        }
      ],
      "source": [
        "print(len(response['embedding']))\n",
        "print(response['embedding'][0 : 9])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAXWlBXTZ2Y8"
      },
      "source": [
        "# Open AI\n",
        "\n",
        "Here we show how to use the OpenAI API to access GPT models. First, you need to sign up for an account at [OpenAI](https://platform.openai.com/signup) and create an API key. Store this key securely as mentioned above. I'm specifically using `0.28.1` here. Instructions for both OpenAI and Gemini may have changed since this was written so refer to their documentation for the latest instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai==0.28.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we get the key from our colab secrets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "gpt_key = userdata.get('a2cps_gpt_api_key')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now making the call to OpenAI is easy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"list out the benefits of having a woodchuck as a pet\" \n",
        "\n",
        "openai.api_key = gpt_key\n",
        "\n",
        "# Call the ChatCompletion endpoint\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4o\", #other examples: \"gpt-3.5-turbo\",\"gpt-4o\", \"gpt-3.5-turbo\"\n",
        "    messages=[\n",
        "        # you can add system level instructions here; I'm omitting for simplicity\n",
        "        #{\"role\": \"system\", \"content\": text_content},\n",
        "        {\"role\": \"user\", \"content\": prompt2}\n",
        "    ],\n",
        "    ## Temperature controls randomness of output\n",
        "    temperature=0.7,\n",
        "    ## The maximum number of tokens to generate in the completion\n",
        "    ## Remember billing is based on input + output tokens totals\n",
        "    max_tokens=4096\n",
        ")\n",
        "# Print the response\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
